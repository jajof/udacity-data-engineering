# Project purpose
This project consists of an ETL process and subsequent analysis of Sparkify, a startup dedicated to streaming music.

Due to Sparkify's great success, the platform is increasingly used by users, generating a significant amount of new data. Specifically, in this project, we focus on data related to the songs users play.

The obvious purpose of this database is to maintain traceability of user actions. However, that is only part of it. The real value of keeping the data organized and accessible comes when we generate value from it. This is achieved by delivering the data to BI teams to better communicate the startup's performance: how we are performing in relation to goals, the impact of new artists, etc. Going further, the data can be provided to Data Science teams, opening the door to possible models, such as predicting and preventing client churn or predicting app traffic.

Thus, having this database allows Sparkify to achieve its goals through the analysis of the data generated by the customers themselves.


# Data modeling
Initially, there are two datasets from the application: event logs and a song dictionary. It is decided to transform and model this with a star schema because:

- It reduces the number of Joins due to its denormalized form.
- It allows for simplified and easy-to-understand queries.
- It facilitates intuitive and direct data analysis.
In this particular case, we have a fact table that contains data on various playback events. Then, we have four dimension tables that provide details about the data in the fact table (Users, Time, Songs, and Artists). Of course, the fact table uses the primary keys of the dimension tables as foreign keys.

# Files Estructure
Next, its the detail of the project files:
- **Configs**: Folder containing two configuration files necessary to run the procedures.
    - **dwh.cfg**: Contains exclusive data for creating the Redshift cluster.
    - **dwh-2.cfg**: Contains exclusive data for connecting to the Redshift cluster.
- **Sample files**: Folder containing samples of the files to be imported from S3.
    - **sample_log.json**: Sample of the events file.
    - **sample_log_json_path.json**: JSON path file for events.
    - **sample_song.json**: Sample of the songs file.
- **1_workspace_preparation.ipynb**: Notebook that creates the Redshift cluster and saves the connection data in **dwh-2.cfg**. To run, it requires an AWS key in **dwh.cfg**.
- **2_sql_queries_writing.ipynb**: Notebook used to write the **sql_queries.py** file.
- **3_main.ipynb**: Notebook from which the following three procedures are called. It serves as the equivalent of a command console.
- **1_create_tables.py**: Python script responsible for dropping tables and recreating them according to the queries in **sql_queries.py**.
- **2_etl.py**: Python script responsible for importing data from S3 to Redshift and then inserting the data into the tables of the star schema, following the queries in **sql_queries.py**.
- **3_analysis.py**: Script responsible for generating a review of the loaded data. It counts the rows in each table and performs additional analysis.
- **sql_queries.py**: File containing all the queries that need to be executed on the cluster. It is called from the three preceding files and is written from **2_sql_queries_writing.ipynb**.


If you want to run the project, you will need to store the aws keys in dwh.cfg. Later, you will have to exec the cells in **1_workspace_preparation.ipynb**. The notebook provides specific instructions to run it with out errors. Later, you have to exec **3_main.ipynb** to get the data and insert it into the operative tables.



